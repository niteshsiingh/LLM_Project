{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to check if GPU is available or not, however it is restricted for Nvidia's GPU only\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk==3.8.1 rouge-score bert-score sentence-transformers meteor\n",
    "!pip install --upgrade nltk==3.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(\"NLTK version:\", nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Additional Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import rouge_score\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from bert_score import score as bert_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the CONAN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the conan dataset\n",
    "with open('CONAN.json', 'r') as f:\n",
    "    conan_data = json.load(f)\n",
    "conan_data = conan_data['conan']\n",
    "\n",
    "df = pd.DataFrame(conan_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos_token to avoid errors\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if multiple GPUs are available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Move the model to the selected device(s)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Device Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class CONANDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=256):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        hate_speech = self.df['hateSpeech'][idx]\n",
    "        counterspeech = self.df['counterSpeech'][idx]\n",
    "        input_text = hate_speech + self.tokenizer.eos_token + counterspeech\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            input_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # Set labels for hate speech tokens to -100\n",
    "        hate_speech_encoding = self.tokenizer(\n",
    "            hate_speech + self.tokenizer.eos_token,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        hs_len = hate_speech_encoding['input_ids'].size(1)\n",
    "        labels[:hs_len] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Custom Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader with a custom collate function\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Train, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into train, validation, and test sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Training Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configurations\n",
    "configs = [\n",
    "    {'learning_rate': 5e-5, 'batch_size': 4, 'num_epochs': 3},\n",
    "    {'learning_rate': 3e-5, 'batch_size': 2, 'num_epochs': 3},\n",
    "    {'learning_rate': 1e-5, 'batch_size': 2, 'num_epochs': 3},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = []\n",
    "gen_token = []\n",
    "ref_token = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute evaluation metrics\n",
    "def evaluate_model(model, test_dataset, tokenizer, device, cnt):\n",
    "    # Generate counterspeech for test set and compute evaluation metrics\n",
    "    model.eval()\n",
    "    generated_texts = []\n",
    "    reference_texts = []\n",
    "    \n",
    "    for idx in range(len(test_dataset)):\n",
    "        hate_speech = test_dataset.df['hateSpeech'][idx]\n",
    "        reference_counterspeech = test_dataset.df['counterSpeech'][idx]\n",
    "\n",
    "        encoding = tokenizer(\n",
    "            hate_speech + tokenizer.eos_token,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            padding='max_length'\n",
    "        ).to(device)\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "\n",
    "        # Generate counterspeech\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=50,\n",
    "                num_beams=5,\n",
    "                no_repeat_ngram_size=2,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        generated_counterspeech = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Remove the hate speech part from the generated text\n",
    "        if generated_counterspeech.startswith(hate_speech):\n",
    "            generated_counterspeech = generated_counterspeech[len(hate_speech):].strip()\n",
    "\n",
    "        generated_texts.append(generated_counterspeech)\n",
    "        reference_texts.append(reference_counterspeech)\n",
    "        break\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    # For BLEU, ROUGE, METEOR, BERTScore, and Cosine Similarity\n",
    "    gen_token.append(generated_texts)\n",
    "    ref_token.append(reference_texts)\n",
    "    \n",
    "    # Initialize lists to store scores\n",
    "    bleu_scores = []\n",
    "    meteor_scores = []\n",
    "    bert_scores = []\n",
    "    cosine_similarities = []\n",
    "\n",
    "    # Initialize the ROUGE scorer\n",
    "    rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "\n",
    "    # Initialize the sentence transformer model for embeddings\n",
    "    st_model = SentenceTransformer('all-MiniLM-L6-v2')  # Or any other appropriate model\n",
    "\n",
    "    # Compute BERTScore\n",
    "    P, R, F1 = bert_score(\n",
    "                generated_texts,\n",
    "                reference_texts,\n",
    "                model_type='bert-base-uncased',\n",
    "                lang='en',\n",
    "                rescale_with_baseline=True,\n",
    "                batch_size=4\n",
    "            )\n",
    "    \n",
    "    smoothing_fn = SmoothingFunction().method1\n",
    "    for i in range(len(generated_texts)):\n",
    "        gen = generated_texts[i]\n",
    "        ref = reference_texts[i]\n",
    "\n",
    "        # BLEU score\n",
    "        reference_tokens = nltk.word_tokenize(ref.lower())\n",
    "        generated_tokens = nltk.word_tokenize(gen.lower())\n",
    "        bleu = sentence_bleu([reference_tokens], generated_tokens, smoothing_function=smoothing_fn)\n",
    "        bleu_scores.append(bleu)\n",
    "        \n",
    "        # ROUGE score\n",
    "        rouge = rouge_scorer_obj.score(ref, gen)\n",
    "        rouge1_scores.append(rouge['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(rouge['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(rouge['rougeL'].fmeasure)\n",
    "\n",
    "        # METEOR score\n",
    "        # Tokenize the reference and generated texts\n",
    "        reference_tokens = nltk.word_tokenize(ref.lower())\n",
    "        generated_tokens = nltk.word_tokenize(gen.lower())\n",
    "\n",
    "        # Compute METEOR score with tokenized inputs\n",
    "        meteor = meteor_score([reference_tokens], generated_tokens)\n",
    "        meteor_scores.append(meteor)\n",
    "\n",
    "        # Cosine similarity\n",
    "        embeddings = st_model.encode([gen, ref])\n",
    "        cosine_similarity = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))\n",
    "        cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "    # BERTScore is already computed as arrays P, R, F1\n",
    "    bert_scores = F1.tolist()  # Convert tensor to list\n",
    "\n",
    "    # Compute average scores\n",
    "    avg_bleu = np.mean(bleu_scores)\n",
    "    avg_meteor = np.mean(meteor_scores)\n",
    "    avg_rouge1 = np.mean(rouge1_scores)\n",
    "    avg_rouge2 = np.mean(rouge2_scores)\n",
    "    avg_rougeL = np.mean(rougeL_scores)\n",
    "    avg_bert_score = np.mean(bert_scores)\n",
    "    avg_cosine_similarity = np.mean(cosine_similarities)\n",
    "\n",
    "    evaluation_scores = {\n",
    "        'BLEU': avg_bleu,\n",
    "        'METEOR': avg_meteor,\n",
    "        'ROUGE-1': avg_rouge1,\n",
    "        'ROUGE-2': avg_rouge2,\n",
    "        'ROUGE-L': avg_rougeL,\n",
    "        'BERTScore': avg_bert_score,\n",
    "        'CosineSimilarity': avg_cosine_similarity\n",
    "    }\n",
    "\n",
    "    return evaluation_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear GPU Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop for Different Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for config in configs:\n",
    "    print(f\"\\nTraining with configuration: {config}\")\n",
    "\n",
    "    config_label = f\"LR_{config['learning_rate']}_BS_{config['batch_size']}_E_{config['num_epochs']}\"\n",
    "\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos_token to avoid errors\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "\n",
    "    # Check if multiple GPUs are available\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Move the model to the selected device(s)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Instantiate datasets and dataloaders with the appropriate batch_size\n",
    "    train_dataset = CONANDataset(train_df, tokenizer)\n",
    "    val_dataset = CONANDataset(val_df, tokenizer)\n",
    "    test_dataset = CONANDataset(test_df, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize optimizer and scaler with the learning rate\n",
    "    optimizer = AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "    num_epochs = config['num_epochs']\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            with autocast():  # Enable mixed precision\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_loss}\")\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_batch in val_dataloader:\n",
    "                val_input_ids = val_batch['input_ids'].to(device)\n",
    "                val_attention_mask = val_batch['attention_mask'].to(device)\n",
    "                val_labels = val_batch['labels'].to(device)\n",
    "\n",
    "                val_outputs = model(val_input_ids, attention_mask=val_attention_mask, labels=val_labels)\n",
    "                val_loss += val_outputs.loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss}\")\n",
    "        model.train()  # Set back to training mode\n",
    "\n",
    "    #Save the trained model locally\n",
    "    config_label = f\"LR_{config['learning_rate']}_BS_{config['batch_size']}_E_{config['num_epochs']}\"\n",
    "    if not os.path.exists('saved_models'):\n",
    "        os.makedirs('saved_models')\n",
    "    model_save_path = os.path.join('saved_models', config_label)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model_to_save = model.module  # For DataParallel models\n",
    "    else:\n",
    "        model_to_save = model\n",
    "    model_to_save.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    \n",
    "    evaluation_scores = evaluate_model(model, test_dataset, tokenizer, device, cnt)\n",
    "    print(f\"Evaluation scores for configuration {config}: {evaluation_scores}\")\n",
    "    cnt+=1\n",
    "\n",
    "    # Store the results\n",
    "    evaluation_results.append({\n",
    "        'config': config,\n",
    "        'evaluation_scores': evaluation_scores,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'config_label': config_label\n",
    "    })\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Freed GPU memory after processing configuration: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate results for all the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'learning_rate': res['config']['learning_rate'],\n",
    "        'batch_size': res['config']['batch_size'],\n",
    "        'num_epochs': res['config']['num_epochs'],\n",
    "        'BLEU': res['evaluation_scores']['BLEU'],\n",
    "        'METEOR': res['evaluation_scores']['METEOR'],\n",
    "        'ROUGE-1': res['evaluation_scores']['ROUGE-1'],\n",
    "        'ROUGE-2': res['evaluation_scores']['ROUGE-2'],\n",
    "        'ROUGE-L': res['evaluation_scores']['ROUGE-L'],\n",
    "        'BERTScore': res['evaluation_scores']['BERTScore'],\n",
    "        'CosineSimilarity': res['evaluation_scores']['CosineSimilarity'],\n",
    "        'config_label': res['config_label']\n",
    "    }\n",
    "    for res in evaluation_results\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['BLEU', 'METEOR', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BERTScore', 'CosineSimilarity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Directory for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('plots'):\n",
    "    os.makedirs('plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 'model_name' column\n",
    "results_df['model_name'] = ['Model-1', 'Model-2', 'Model-3']\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(results_df['model_name'], results_df[metric], color='green')\n",
    "    plt.title(f'{metric} for Different Models')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel(metric)\n",
    "    plt.xticks(rotation=0)  # No rotation needed for short labels\n",
    "    plt.tight_layout()\n",
    "    plot_filename = f'plots/{metric}_scores.png'\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.show()\n",
    "    print(f\"Plot saved to {plot_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
